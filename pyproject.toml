[project]
name = "WeClone"
version = "0.3.02"
description = "One-stop solution for creating your digital avatar from chat history"
authors = [{ name = "xming521" }]
readme = "README.md"
requires-python = ">=3.10,<3.11"

dependencies = [
  "pandas",
  "pyjson5",
  "omegaconf",
  "click",
  "tqdm",
  "pydantic==2.10.6",
  "setuptools>=78.1.0",
  "loguru>=0.7.3",
  "tomli; python_version < '3.11'",
  "langchain",
  "openai==1.87.0",
  "pip"
]

[tool.weclone]
# Configuration file version number. This number should be incremented when the configuration file structure or important default values change.
config_version = "0.3.02"

config_changelog = """
[0.2.24] - 2025-06-19 - add test_model_args and vllm_args.
[0.3.00] - 2025-06-30 - Support TG chat logs, add language parameter, add log level parameter.
[0.3.02] - 2025-08-15 - Allow the use of the enable_thinking to control offline cleaning..
"""

[dependency-groups]
main = [
  "llamafactory @ git+https://github.com/hiyouga/LLaMA-Factory.git",
  "vllm==0.10.0; platform_system == 'Linux'",
  "torch==2.7.1",
  "torchvision==0.22.1",
  "transformers==4.53.2",
  "accelerate==1.7.0",
  "triton==3.3.1; platform_system == 'Linux'",
  "presidio_analyzer[transformers]",
  "presidio_anonymizer",
]
sparktts = [
  "einops>=0.8.1",
  "einx>=0.3.0",
  "numpy==1.26.4",
  "omegaconf>=2.3.0",
  "packaging>=24.2",
  "safetensors>=0.5.2",
  "soundfile>=0.12.1",
  "soxr>=0.5.0.post1",
  "torchaudio>=2.6.0",
  "tqdm>=4.66.5",
]

dev = ["pytest", "pytest-order", "pyright", "ruff", "pre-commit"]

[project.scripts]
weclone-cli = "weclone.cli:cli"

[tool.uv]

[tool.uv.pip]
torch-backend = "auto"

[tool.uv.sources]
torch = [
  { index = "pytorch-cu126", marker = "platform_system == 'Windows'" },
  { index = "pytorch-cu126", marker = "platform_system == 'Linux'" },
]
torchaudio = [
  { index = "pytorch-cu126", marker = "platform_system == 'Windows'" },
  { index = "pytorch-cu126", marker = "platform_system == 'Linux'" },
]
torchvision = [
  { index = "pytorch-cu126", marker = "platform_system == 'Windows'" },
  { index = "pytorch-cu126", marker = "platform_system == 'Linux'" },
]
triton = [
  { index = "pytorch-cu126", marker = "platform_system == 'Windows'" },
  { index = "pytorch-cu126", marker = "platform_system == 'Linux'" },
]


[[tool.uv.index]]
name = "pytorch-cu126"
url = "https://download.pytorch.org/whl/cu126"
explicit = true

[tool.setuptools.packages.find]
where = ["."]                      
include = ["weclone*"]             
exclude = ["*tests*", "*archive*"]


[tool.pyright]
typeCheckingMode = "basic"
include = ["weclone/data"]
exclude = ["**/archive", "**/tests"]
ignore = ["**/archive"]

reportMissingImports = "error"
reportMissingTypeStubs = false

pythonVersion = "3.10"
pythonPlatform = "Linux"

[tool.ruff]
exclude = [
  "**/archive",
  "**/tests",
  "weclone-audio/src/server未完工",
  "weclone-audio/src/Spark-TTS",
]
line-length = 110

lint.ignore = ["F403", "F405", "E501", "E402"]
lint.select = [
  "F",     # Pyflakes
  "W",     # pycodestyle warnings
  "E",     # pycodestyle errors
  "ASYNC", # flake8-async
  "C4",    # flake8-comprehensions
  "Q",     # flake8-quotes
]
target-version = "py310"

[tool.pytest.ini_options]
addopts = "-x -v -s --tb=short"
